<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building a Parallel LLM Inference API During the Cloudflare Outage | BerozgaarCoder</title>
    <meta name="description" content="How I built a scalable parallel LLM inference system using Modal.com during the Cloudflare outage - processing 1000 prompts across multiple models with GPU containers and intelligent queuing.">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://berozgaarcoder.com/posts/building-parallel-llm-inference-api.html">
    <meta property="og:title" content="Building a Parallel LLM Inference API During the Cloudflare Outage | BerozgaarCoder">
    <meta property="og:description" content="How I built a scalable parallel LLM inference system using Modal.com during the Cloudflare outage - processing 1000 prompts across multiple models with GPU containers and intelligent queuing.">
    <meta property="og:image" content="../images/llm.png">
    <meta property="article:published_time" content="2025-11-18T10:00:00+05:30">
    <meta property="article:author" content="Aniket">

    <!-- LinkedIn -->
    <meta property="linkedin:title" content="Building a Parallel LLM Inference API During the Cloudflare Outage">
    <meta property="linkedin:description" content="How I built a scalable parallel LLM inference system using Modal.com during the Cloudflare outage.">
    <meta property="linkedin:image" content="https://berozgaarcoder.com/">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:url" content="https://berozgaarcoder.com/posts/building-parallel-llm-inference-api.html">
    <meta name="twitter:title" content="Building a Parallel LLM Inference API During the Cloudflare Outage">
    <meta name="twitter:description" content="How I built a scalable parallel LLM inference system using Modal.com during the Cloudflare outage.">
    <meta name="twitter:image" content="https://berozgaarcoder.com/images/llm.png">

    <!-- Stylesheets -->
    <link rel="stylesheet" href="/css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>
    <header class="site-header">
        <div class="container">
            <div class="logo">
                <a href="/">
                    <span class="logo-text">BerozgaarCoder</span>
                </a>
            </div>
            <p class="tagline">Code Ka Jugaad</p>
            
            <nav class="main-nav">
                <ul>
                    <li><a href="/">Home</a></li>
                    <li><a href="/posts">Blog</a></li>
                    <li><a href="/about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="site-main">
        <article class="blog-post">
            <header class="post-header">
                <h1>Building a Parallel LLM Inference API During the Cloudflare Outage</h1>
                <div class="post-meta">
                    <time datetime="2025-11-18">November 18, 2025</time>
                    <span class="post-author">by Aniket</span>
                    <span class="post-category">AI/ML</span>
                </div>
                <img src="/images/llm.png" alt="Parallel LLM inference architecture" class="post-featured-image" onerror="this.style.display='none'">
            </header>

            <div class="post-content">
                <h2>When Life Gives You Downtime...</h2>

                <p>Today started like any other Wednesday until Cloudflare decided to take an unexpected nap, bringing down a significant chunk of the internet with it. As services across the web ground to a halt, I found myself with an unexpected gift â€“ uninterrupted time to work on something interesting. My mentor Konark, whom I've known since my college days, had given me an intriguing problem statement a few weeks ago, and today felt like the perfect opportunity to dive deep into it.</p>

                <h2>The Challenge: Parallel LLM Inference at Scale</h2>

                <p>The problem Konark posed was deceptively simple: "How do you run 1000 different prompts across multiple LLM models efficiently, without managing infrastructure?" The real-world use case was clear â€“ imagine you're testing different models for your application, or you need to run the same prompt through multiple models to compare outputs, or you're doing batch inference for a research project. You need something that scales automatically, handles queuing intelligently, and doesn't require you to provision servers or worry about GPU availability.</p>

                <p>The requirements were straightforward but challenging: take a JSON file with prompts and their target models, parse it, queue the prompts by model type, run them in parallel across multiple GPU containers, and return results as they complete. Oh, and it should auto-scale based on load and have configurable limits to control costs. Simple, right?</p>

                <h2>Enter Modal.com: Serverless Infrastructure for the AI Era</h2>

                <p>Before diving into the solution, let me talk about Modal â€“ the platform that made this entire project possible. Modal (modal.com) is a serverless compute platform specifically designed for data and AI workloads. Think of it as AWS Lambda, but built from the ground up for machine learning, with native GPU support, container orchestration, and a Python-first API that feels almost magical.</p>

                <p>What makes Modal special is how it handles the complexity of running ML workloads. You write Python code, decorate it with Modal's function decorators, and suddenly your local functions become distributed, auto-scaling cloud functions. Need a GPU? Add <code>gpu="T4"</code> to your decorator. Want to limit concurrent containers? Add <code>concurrency_limit=4</code>. Need persistent storage across invocations? Mount a Modal Volume. It's infrastructure-as-code meets serverless meets ML-ops, all wrapped in a delightfully simple API.</p>

                <p>Modal handles the hard parts â€“ container orchestration, GPU scheduling, automatic scaling, cold start optimization, and even model caching. You focus on the logic, Modal handles the infrastructure. For AI workloads, especially LLM inference, this is transformative. No Kubernetes manifests, no Docker registries, no EC2 instances to babysit. Just Python and a declarative way to say "run this on GPUs in the cloud."</p>

                <h2>The Solution: Model-Specific Queues and Parallel Processing</h2>

                <p>The architecture I built leverages Modal's Queue system and GPU containers to create a truly parallel inference pipeline. Here's how it works: when you submit a JSON file with prompts, the API parses it and groups prompts by their target model (llama3.2, phi3, or mistral in my POC). Each model gets its own dedicated Modal Queue â€“ essentially a distributed FIFO queue that persists across container restarts.</p>

                <p>The magic happens in the parallelization. For each model type, I created a dedicated Ollama service running in a GPU container. These are long-lived containers (Modal calls them "classes") that start an Ollama server on boot, pull the model once, and then handle inference requests. The <code>@modal.enter()</code> decorator runs the initialization code when the container starts, making subsequent inferences blazingly fast since the model is already loaded in GPU memory.</p>

                <p>Worker functions continuously poll their respective queues and dispatch inference requests to the appropriate Ollama service. Since Modal allows you to set <code>concurrency_limit</code> per function and class, I can precisely control how many GPU containers spin up per model type. Set it to 1 for cost-conscious testing, bump it to 4 for production scale. All three model types run completely in parallel â€“ llama3.2, phi3, and mistral each have their own queue, their own workers, their own GPU containers, all processing simultaneously.</p>

                <h2>The Fun Parts: Wrestling with Distributed Systems</h2>

                <p>Building this was genuinely fun, especially the debugging dance. The first iteration had all prompts going to a single queue, which defeated the purpose of parallel processing. Then I had the "aha moment" â€“ separate queues per model! This meant llama could chew through 334 prompts on its 4 GPU containers while phi and mistral did the same on theirs, all at the same time.</p>

                <p>The JSON format was another fun gotcha. I initially had the API expecting a raw array, but FastAPI's Pydantic validation wanted a proper object with a "prompts" key. One quick fix later, and curl was happy. Then came the <code>'Function' object is not callable</code> error â€“ I was calling <code>service.infer(prompt)</code> instead of <code>service.infer.remote(prompt)</code>. Modal's remote execution model requires the <code>.remote()</code> suffix for methods on Modal classes. Once I grokked that pattern, everything clicked.</p>

                <p>Watching the logs as containers spun up was oddly satisfying. You'd see "ðŸš€ Starting Ollama server on T4 GPU..." followed by model pulls, then the steady stream of "Processing llama3.2: prompt_123" messages. The first run took about 3-4 minutes as each model downloaded (~4GB each), but subsequent runs were instant since Modal caches everything. That's the kind of developer experience that makes you smile.</p>

                <h2>Configurability: Making It Actually Useful</h2>

                <p>One of Konark's implicit requirements was making this practical for different scenarios â€“ testing, development, production, cost-sensitive deployments. I solved this with a simple configuration block at the top of the Modal app:</p>

                <pre><code>GPU_CONFIG = {
    "type": "T4",                    # GPU type
    "max_containers_per_model": 1,   # How many GPUs per model
    "idle_timeout": 600              # Keep warm for reuse
}</code></pre>

                <p>Want to test with minimal cost? Set <code>max_containers_per_model=1</code> and use T4 GPUs (~$1.80/hour for all 3 models). Ready for production? Bump it to 4 and switch to A10Gs. Need CPU-only for some reason? Set <code>type=None</code>. The entire scaling behavior changes with a few lines of configuration, no code changes required.</p>

                <p>This configurability extends to the models themselves. Adding a new model is straightforward â€“ create a new Ollama service class, add a queue for it, wire up a worker function, and you're done. The pattern is consistent across all models, so it's almost copy-paste-modify. I can see this evolving into a more dynamic system where you just list model names in config and the code generates the necessary infrastructure automatically.</p>

                <h2>Results: A Working POC</h2>

                <p>After a few hours of coding and debugging (thank you, Cloudflare outage), I had a working system. Submit a JSON with 10 prompts split across 3 models, and you get results back in about 30-40 seconds (including cold start). Submit 1000 prompts with <code>max_containers_per_model=4</code>, and it's done in about 4-5 minutes. The API returns a job ID immediately, you poll for results, and they stream back as they complete. Simple, scalable, and surprisingly cost-effective.</p>

                <p>The repo (github.com/aniketmaithani/modal_poc) has everything you need to run this yourself â€“ the Modal app, a Python client, sample JSON files with 10 and 1000 prompts, and a detailed README. It's very much a POC, rough around the edges, but it works and demonstrates the core concepts beautifully.</p>

                <h2>What's Next: Turning POC into Library</h2>

                <p>This is just the beginning. The current implementation is model-specific (hardcoded for llama3.2, phi3, mistral), but it's begging to be generalized. I envision a library where you just pass a config like <code>{"models": ["llama3.2", "codellama", "mixtral"]}</code> and it dynamically creates the queues, services, and workers. Add some retry logic, proper error handling, dead letter queues for failed prompts, and you've got something genuinely production-ready.</p>

                <p>There's also the monitoring aspect â€“ right now you're flying blind once you submit a job. Adding real-time progress tracking, per-model metrics, cost estimation, and maybe even a simple dashboard would make this much more usable. Modal's built-in observability is good, but custom metrics specific to LLM inference (tokens/sec, model utilization, queue depth over time) would be invaluable.</p>

                <p>Another direction is supporting more than just Ollama. What about OpenAI API calls? Anthropic Claude? HuggingFace models? The queue-based architecture is model-agnostic â€“ you just need different service implementations. I could see a plugin system where you register model providers and the framework handles the rest.</p>

                <h2>Reflections: The Joy of Building</h2>

                <p>Working on this today was a reminder of why I love programming. Take an interesting problem, learn a new platform (Modal is genuinely impressive), battle through the inevitable bugs, and emerge with something that actually works. The fact that it happened during a major internet outage just adds to the story. While everyone else was refreshing their dashboards and cursing Cloudflare, I was knee-deep in distributed queues and GPU containers, completely oblivious to the chaos outside my terminal.</p>

                <p>Konark's problem statement was simple on the surface but rich with complexity underneath â€“ exactly the kind of challenge that makes you think differently about system design. How do you make parallel processing feel effortless? How do you hide infrastructure complexity without sacrificing control? How do you build something that works for both the hobbyist running 10 prompts and the company running 10,000?</p>

                <p>This POC doesn't answer all those questions, but it's a solid start. It proves the concept, validates the architecture, and most importantly, it's fun to use. That last part matters more than people realize â€“ developer experience is a feature. If your tool makes developers smile, you're doing something right.</p>

                <h2>Try It Yourself</h2>

                <p>If you're curious, the code is on GitHub at github.com/aniketmaithani/modal_poc. You'll need a Modal account (they have a generous free tier), and you can have this running in about 5 minutes. The README has everything â€“ setup instructions, configuration options, sample commands. Try it with 10 prompts first, then scale up to 1000. Watch the logs, tweak the GPU config, add your own models. Break it, fix it, make it yours.</p>

                <p>Thanks, Konark, for the problem statement. Thanks, Cloudflare, for the downtime. And thanks, Modal, for making distributed GPU computing feel like magic.</p>
            </div>

            <footer class="post-footer">
                <div class="post-tags">
                    <a href="/tag/ai">#ai</a>
                    <a href="/tag/llm">#llm</a>
                    <a href="/tag/modal">#modal</a>
                    <a href="/tag/python">#python</a>
                    <a href="/tag/distributed-systems">#distributed-systems</a>
                    <a href="/tag/gpu">#gpu</a>
                    <a href="/tag/serverless">#serverless</a>
                </div>
                <div class="post-share">
                    <h3>Share this post:</h3>
                    <a href="https://twitter.com/intent/tweet?text=Building a Parallel LLM Inference API During the Cloudflare Outage&url=https://berozgaarcoder.com/posts/building-parallel-llm-inference-api.html" target="_blank" rel="noopener noreferrer">Twitter</a>
                    <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://berozgaarcoder.com/posts/building-parallel-llm-inference-api.html" target="_blank" rel="noopener noreferrer">LinkedIn</a>
                </div>
            </footer>
        </article>

        <section class="comments">
            <h3>Comments</h3>
            <p>Comments coming soon! For now, feel free to reach out on <a href="https://twitter.com/berozgaarcoder" target="_blank">Twitter</a> or <a href="https://linkedin.com/in/aniketmaithani" target="_blank">LinkedIn</a>.</p>
        </section>
    </main>

    <footer class="site-footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-logo">
                    <span class="logo-text">BerozgaarCoder</span>
                    <p class="tagline">Code Ka Jugaad</p>
                </div>
                
                <div class="footer-links">
                    <h3>Quick Links</h3>
                    <ul>
                        <li><a href="/" class="footer-link">Home</a></li>
                        <li><a href="/posts" class="footer-link">Blog</a></li>
                        <li><a href="/about.html" class="footer-link">About</a></li>
                    </ul>
                </div>
                
                <div class="footer-social">
                    <h3>Connect With Me</h3>
                    <ul class="social-links">
                        <li><a href="https://github.com/aniketmaithani" target="_blank" rel="noopener" aria-label="GitHub Profile" class="social-link">GitHub</a></li>
                        <li><a href="https://twitter.com/2aniketmaithani" target="_blank" rel="noopener" aria-label="Twitter Profile" class="social-link">Twitter</a></li>
                        <li><a href="https://linkedin.com/in/aniketmaithani" target="_blank" rel="noopener" aria-label="LinkedIn Profile" class="social-link">LinkedIn</a></li>
                    </ul>
                </div>
            </div>
            
            <div class="footer-bottom">
                <p class="copyright">&copy; 2025 BerozgaarCoder. All rights reserved.</p>
                <p class="footer-description">This blog is dedicated to sharing coding knowledge, personal experiences, and technical insights. All content is original unless otherwise stated.</p>
                <p class="last-updated">Last updated: <time datetime="2025-11-18">November 18, 2025</time></p>
            </div>
        </div>
    </footer>

    <script src="/js/main.js"></script>
</body>
</html>